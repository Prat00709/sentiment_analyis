"""
Streamlit-ready preprocessing script (in-place modified).
This file exposes run_preprocessing(save_folder="dashboard_outputs") so Streamlit can import and call it directly.
The original script body is executed inside that function to preserve behavior when run as a script.
"""

import os
from pathlib import Path
import sys

# Original script content embedded as a string and executed inside run_preprocessing()
_ORIG_SCRIPT = '#!/usr/bin/env python\n# coding: utf-8\n\n# In[ ]:\n\n\n# Corrected Flipkart scraper (requests + BeautifulSoup) with relative-date parsing\n# Paste this into Jupyter and run. Update CSS_* variables if necessary.\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse, urlunparse, parse_qs, urlencode\nimport time, random, re, logging\nfrom dateutil import parser as dateparser\nfrom dateutil.relativedelta import relativedelta\nfrom datetime import datetime, timedelta, date\nimport pandas as pd\nfrom collections import OrderedDict\n\nlogging.basicConfig(level=logging.WARNING)\n\n# ---------------------------\n# >>> EDIT THESE SELECTORS IF NEEDED <<<\n# ---------------------------\nCSS_REVIEW_CARD      = "col EPCmJX Ma1fCG"   # outer container class for a single review card\nCSS_RATING           = "XQDdHH Ga3i8K"      # element/class containing rating (text like \'5\')\nCSS_TITLE            = "z9E0IG"             # review title\nCSS_REVIEW_TEXT      = "ZmyHeo"             # main review text block\nCSS_REVIEWER_NAME    = "_2NsDsF AwS1CA"     # reviewer display name\nCSS_VERIFIED         = "Certified Buyer"    # text to detect verified purchase (not a class)\nCSS_LOCATION         = "MztJPv"             # optional: class that may contain location\nCSS_DATE             = "_2NsDsF"            # optional: class that may contain date or relative time\n\n# ---------------------------\n# Config\n# ---------------------------\nHEADERS = {\n    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "\n                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",\n    "Accept-Language": "en-US,en;q=0.9",\n    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"\n}\nREQUEST_DELAY = (1.5, 3.0)\nMAX_PAGES = 500\nUSER_AGENTS = [\n    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",\n    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Safari/605.1.15",\n    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",\n]\n\nsession = requests.Session()\nsession.headers.update(HEADERS)\n\n# ---------------------------\n# Utilities: build URL & choose UA\n# ---------------------------\ndef build_page_url(base_url, page_num):\n    parsed = urlparse(base_url)\n    qs = parse_qs(parsed.query)\n    qs["page"] = [str(page_num)]\n    new_q = urlencode(qs, doseq=True)\n    return urlunparse((parsed.scheme, parsed.netloc, parsed.path, \'\', new_q, \'\'))\n\ndef choose_ua():\n    return random.choice(USER_AGENTS)\n\n# ---------------------------\n# Robust fetch with retries and 429 handling\n# ---------------------------\ndef fetch_with_retries(session, url, timeout=18, max_retries=6, base_backoff=1.0, max_backoff=60.0,\n                       rotate_ua=True, proxies=None):\n    attempt = 0\n    proxy_list = proxies if isinstance(proxies, (list, tuple)) else ([proxies] if proxies else None)\n    last_exception = None\n\n    while attempt < max_retries:\n        attempt += 1\n        if rotate_ua:\n            session.headers.update({"User-Agent": choose_ua()})\n        proxy = random.choice(proxy_list) if proxy_list else None\n\n        try:\n            resp = session.get(url, timeout=timeout, proxies=proxy)\n        except Exception as e:\n            last_exception = e\n            wait = min(max_backoff, base_backoff * (2 ** (attempt - 1)))\n            wait = wait * (0.5 + random.random())\n            logging.warning(f"Request exception (attempt {attempt}): {e}. Sleeping {wait:.1f}s")\n            time.sleep(wait)\n            continue\n\n        if resp.status_code == 200:\n            return resp\n\n        if resp.status_code == 429:\n            ra = resp.headers.get("Retry-After")\n            if ra:\n                try:\n                    wait = int(ra)\n                except:\n                    wait = base_backoff * (2 ** (attempt - 1))\n            else:\n                wait = base_backoff * (2 ** (attempt - 1))\n            wait = min(max_backoff, wait + random.uniform(0.5, 2.0))\n            logging.warning(f"429 received. Sleeping {wait:.1f}s (attempt {attempt})")\n            time.sleep(wait)\n            continue\n\n        if 500 <= resp.status_code < 600:\n            wait = min(max_backoff, base_backoff * (2 ** (attempt - 1)))\n            wait = wait * (0.5 + random.random())\n            logging.warning(f"Server error {resp.status_code}. Sleeping {wait:.1f}s")\n            time.sleep(wait)\n            continue\n\n        # other codes: return response for debugging\n        logging.warning(f"Non-retryable status {resp.status_code} returned for {url}")\n        return resp\n\n    logging.error(f"Exhausted retries for {url}; last exception: {last_exception}")\n    return None\n\n# ---------------------------\n# Date parsing: relative date like \'5 days ago\' => real date\n# ---------------------------\nREL_PAT_DAY   = re.compile(r\'(\\d+)\\s+day\', re.I)\nREL_PAT_MONTH = re.compile(r\'(\\d+)\\s+month\', re.I)\nREL_PAT_YEAR  = re.compile(r\'(\\d+)\\s+year\', re.I)\nREL_PAT_HOUR  = re.compile(r\'(\\d+)\\s+hour\', re.I)\nREL_PAT_MIN   = re.compile(r\'(\\d+)\\s+min\', re.I)\nREL_PAT_TODAY = re.compile(r\'\\b(today|just now)\\b\', re.I)\nREL_PAT_YEST  = re.compile(r\'\\b(yesterday)\\b\', re.I)\n\ndef parse_flipkart_relative_date(text, today=None):\n    """Parse phrases like \'5 days ago\', \'3 months ago\', \'1 year ago\' into a date object."""\n    if today is None:\n        today = datetime.today()\n    if not text or not isinstance(text, str):\n        return None\n    s = text.strip().lower()\n\n    # direct absolute date first (if page has full date strings)\n    try:\n        # quick guard: if string contains a year like 2024 or month words, try parse\n        if re.search(r\'\\b\\d{4}\\b\', s) or re.search(r\'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b\', s, re.I):\n            try:\n                dt = dateparser.parse(s, fuzzy=True)\n                if dt:\n                    return dt.date()\n            except:\n                pass\n    except:\n        pass\n\n    # relative patterns\n    m = REL_PAT_DAY.search(s)\n    if m:\n        return (today - timedelta(days=int(m.group(1)))).date()\n    m = REL_PAT_HOUR.search(s)\n    if m:\n        return (today - timedelta(hours=int(m.group(1)))).date()\n    m = REL_PAT_MIN.search(s)\n    if m:\n        return (today - timedelta(minutes=int(m.group(1)))).date()\n    m = REL_PAT_MONTH.search(s)\n    if m:\n        return (today - relativedelta(months=int(m.group(1)))).date()\n    m = REL_PAT_YEAR.search(s)\n    if m:\n        return (today - relativedelta(years=int(m.group(1)))).date()\n    if REL_PAT_TODAY.search(s):\n        return today.date()\n    if REL_PAT_YEST.search(s):\n        return (today - timedelta(days=1)).date()\n\n    # last resort: try to parse fuzzy\n    try:\n        dt = dateparser.parse(s, fuzzy=True)\n        if dt:\n            return dt.date()\n    except:\n        return None\n    return None\n\n# ---------------------------\n# Small helper to clamp dates to reasonable range\n# ---------------------------\ndef clamp_date(d, earliest_year=2000, latest_date=None):\n    if pd.isna(d) or d is None:\n        return None\n    if latest_date is None:\n        latest_date = datetime.today().date()\n    if isinstance(d, datetime):\n        d = d.date()\n    if d.year < earliest_year or d > latest_date:\n        return None\n    return d\n\n# ---------------------------\n# Text extraction helpers\n# ---------------------------\ndef safe_get_text(elem):\n    try:\n        return elem.get_text(" ", strip=True)\n    except:\n        return ""\n\n# ---------------------------\n# Core scraper function\n# ---------------------------\ndef scrape_flipkart_reviews(base_url, max_reviews=600, proxies=None):\n    results = []\n    page = 1\n    pages_checked = 0\n\n    while len(results) < max_reviews and pages_checked < MAX_PAGES:\n        pages_checked += 1\n        url = build_page_url(base_url, page)\n        print(f"Fetching page {page}: {url}")\n        resp = fetch_with_retries(session, url, rotate_ua=True, proxies=proxies)\n        if resp is None:\n            print("Failed to fetch after retries; saved debug and stopping.")\n            with open(f"debug_failed_fetch_page{page}.html", "w", encoding="utf-8") as f:\n                # best-effort: write last response text if present, else empty\n                f.write(resp.text if resp else "")\n            break\n\n        if resp.status_code != 200:\n            debug_fname = f"debug_status_{resp.status_code}_page{page}.html"\n            with open(debug_fname, "w", encoding="utf-8") as f:\n                f.write(resp.text[:300000])\n            print(f"Non-200 status {resp.status_code} â€” saved to {debug_fname}. Stopping.")\n            break\n\n        soup = BeautifulSoup(resp.text, "lxml")\n\n        # --- find cards ---\n        cards = []\n        if CSS_REVIEW_CARD.strip().startswith("."):\n            try:\n                cards = soup.select(CSS_REVIEW_CARD)\n            except Exception:\n                cards = []\n        else:\n            class_parts = CSS_REVIEW_CARD.split()\n            if len(class_parts) == 1:\n                cards = soup.find_all("div", class_=CSS_REVIEW_CARD)\n            else:\n                def has_all_classes(tag):\n                    if tag.name != \'div\': return False\n                    cls = tag.get("class") or []\n                    return all(part in cls for part in class_parts)\n                cards = soup.find_all(has_all_classes)\n\n        if not cards:\n            debug_fname = f"debug_page_no_cards_page{page}.html"\n            with open(debug_fname, "w", encoding="utf-8") as f:\n                f.write(resp.text)\n            print(f"No review cards found on page {page}. Saved full page to {debug_fname}. Stopping.")\n            break\n\n        print(f"Found {len(cards)} card(s) on this page.")\n        for c in cards:\n            # rating\n            rating = None\n            try:\n                if CSS_RATING and CSS_RATING.startswith("."):\n                    r_el = c.select_one(CSS_RATING)\n                else:\n                    parts = CSS_RATING.split()\n                    if len(parts) == 1:\n                        r_el = c.find(attrs={"class": CSS_RATING})\n                    else:\n                        def match_rating(tag):\n                            cls = tag.get("class") or []\n                            return any(part in cls for part in parts)\n                        r_el = c.find(match_rating)\n                rating = float(re.search(r\'(\\d+(\\.\\d+)?)\', safe_get_text(r_el) if r_el else "").group(1)) if r_el and re.search(r\'(\\d+(\\.\\d+)?)\', safe_get_text(r_el)) else None\n            except:\n                rating = None\n\n            # title\n            title = None\n            try:\n                if CSS_TITLE and CSS_TITLE.startswith("."):\n                    t_el = c.select_one(CSS_TITLE)\n                else:\n                    t_el = c.find(attrs={"class": CSS_TITLE})\n                title = safe_get_text(t_el) if t_el else None\n            except:\n                title = None\n\n            # review text\n            review_text = None\n            try:\n                if CSS_REVIEW_TEXT and CSS_REVIEW_TEXT.startswith("."):\n                    rt_el = c.select_one(CSS_REVIEW_TEXT)\n                else:\n                    rt_el = c.find(attrs={"class": CSS_REVIEW_TEXT})\n                if rt_el:\n                    review_text = safe_get_text(rt_el)\n                else:\n                    # fallback: take the longest text candidate inside card\n                    candidates = [safe_get_text(t) for t in c.find_all(["p","div","span"])]\n                    review_text = max(candidates, key=len) if candidates else None\n            except:\n                review_text = None\n\n            # reviewer name\n            reviewer_name = None\n            try:\n                if CSS_REVIEWER_NAME and CSS_REVIEWER_NAME.startswith("."):\n                    rn_el = c.select_one(CSS_REVIEWER_NAME)\n                else:\n                    rn_el = c.find(attrs={"class": CSS_REVIEWER_NAME})\n                reviewer_name = safe_get_text(rn_el) if rn_el else None\n            except:\n                reviewer_name = None\n\n            # full text for searching\n            full_text = safe_get_text(c)\n\n            # verified\n            verified = False\n            if CSS_VERIFIED and isinstance(CSS_VERIFIED, str) and CSS_VERIFIED.strip() != "":\n                if CSS_VERIFIED.lower() in full_text.lower():\n                    verified = True\n            # also check if a small element contains "Certified" text\n            if not verified:\n                if re.search(r\'certified buyer\', full_text, re.I):\n                    verified = True\n\n            # location extraction\n            reviewer_location = None\n            try:\n                m_loc = re.search(r\'Certified Buyer[, ]+\\s*([A-Za-z\\u00C0-\\u017F .\\-\\(\\)]+?)(?:\\s*\\||\\s*-|$)\', full_text, re.I)\n                if m_loc:\n                    reviewer_location = m_loc.group(1).strip()\n                else:\n                    m_loc2 = re.search(r\'Reviewed in\\s+([A-Za-z ,\\-\\(\\)]+?)\\s+(?:on|$)\', full_text, re.I)\n                    if m_loc2:\n                        reviewer_location = m_loc2.group(1).strip()\n                    else:\n                        if CSS_LOCATION:\n                            try:\n                                if CSS_LOCATION.startswith("."):\n                                    loc_el = c.select_one(CSS_LOCATION)\n                                else:\n                                    loc_el = c.find(attrs={"class": CSS_LOCATION})\n                                if loc_el:\n                                    reviewer_location = safe_get_text(loc_el)\n                            except:\n                                pass\n            except:\n                reviewer_location = None\n\n            # date extraction: try CSS_DATE element first (may contain relative text)\n            review_date = None\n            try:\n                date_text = None\n                if CSS_DATE:\n                    try:\n                        if CSS_DATE.startswith("."):\n                            date_elt = c.select_one(CSS_DATE)\n                        else:\n                            date_elt = c.find(attrs={"class": CSS_DATE})\n                        if date_elt:\n                            date_text = safe_get_text(date_elt)\n                    except:\n                        date_text = None\n\n                # if no specific date element, try looking for \'ago\' phrases in full_text\n                if not date_text:\n                    # look for patterns like "5 days ago" or "Reviewed in X on 5 July 2025" within full_text\n                    # we\'ll take a small substring around keywords \'ago\' or \'Reviewed in\' to avoid noise\n                    m_ago = re.search(r\'(\\d+\\s+(?:day|month|year|hour|min)s?\\s+ago)\', full_text, re.I)\n                    if m_ago:\n                        date_text = m_ago.group(1)\n                    else:\n                        # fallback: full_text may contain \'Reviewed in ... on <date>\'\n                        m_rev = re.search(r\'Reviewed in [A-Za-z ,\\-\\(\\)]+ on ([A-Za-z0-9 ,\\-]+)\', full_text, re.I)\n                        if m_rev:\n                            date_text = m_rev.group(1)\n\n                # parse the date_text using relative parser first\n                parsed_date = None\n                if date_text:\n                    parsed_date = parse_flipkart_relative_date(date_text)\n                # fallback: try absolute parsing if relative parser returns None\n                if not parsed_date:\n                    parsed_date = None\n                    if date_text:\n                        try:\n                            parsed_date = dateparser.parse(date_text, fuzzy=True).date()\n                        except:\n                            parsed_date = None\n                # last fallback: try parse from full_text generically\n                if not parsed_date:\n                    try:\n                        maybe = dateparser.parse(full_text, fuzzy=True)\n                        if maybe:\n                            parsed_date = maybe.date()\n                    except:\n                        parsed_date = None\n\n                # clamp dates to sensible range\n                parsed_date = clamp_date(parsed_date)\n                review_date = parsed_date\n            except Exception as e:\n                review_date = None\n\n            results.append({\n                "rating": rating,\n                "title": title,\n                "review_text": review_text,\n                "reviewer_name": reviewer_name,\n                "reviewer_location": reviewer_location,\n                "review_date": review_date.isoformat() if review_date else None,\n                "verified_purchase": bool(verified)\n            })\n\n            if len(results) >= max_reviews:\n                break\n\n        # next page\n        page += 1\n        time.sleep(random.uniform(*REQUEST_DELAY))\n\n    df = pd.DataFrame(results)\n    # normalize rating\n    if \'rating\' in df.columns:\n        df[\'rating\'] = pd.to_numeric(df[\'rating\'], errors=\'coerce\').round(1)\n    # normalize review_date to date dtype\n    if \'review_date\' in df.columns:\n        df[\'review_date\'] = pd.to_datetime(df[\'review_date\'], errors=\'coerce\').dt.date\n    return df\n\n# ---------------------------\n# Run scraper for chosen products and save CSV\n# ---------------------------\nproduct_urls = OrderedDict([\n    ("samsung_a35", "https://www.flipkart.com/samsung-galaxy-a35-5g-awesome-lilac-128-gb/product-reviews/itm5a31476ca6a35?pid=MOBGYT2HDZYASSZG&lid=LSTMOBGYT2HDZYASSZGOUJTDB&marketplace=FLIPKART"),\n    ("iphone_13",  "https://www.flipkart.com/apple-iphone-13-starlight-128-gb/product-reviews/itmc9604f122ae7f?pid=MOBG6VF5ADKHKXFX&lid=LSTMOBG6VF5ADKHKXFX4LCPEV&marketplace=FLIPKART")\n])\n\nMAX_PER_PRODUCT = 700\n\nfor label, url in product_urls.items():\n    print(f"\\n=== Scraping product: {label} ===")\n    df = scrape_flipkart_reviews(url, max_reviews=MAX_PER_PRODUCT)\n    outname = f"{label}_flipkart_reviews_corrected.csv"\n    df.to_csv(outname, index=False, encoding="utf-8")\n    print(f"Saved {len(df)} rows to {outname}")\n\nprint("\\nDone â€” corrected scraper finished. If you still see null/odd dates,")\nprint("paste one review card\'s outerHTML here (right-click -> Inspect -> Copy -> Copy outerHTML) and I will tailor selectors.")\n\n\n# In[ ]:\n\n\n# 2) Load the CSVs (edit filenames if you saved with different names)\ninput_files = {\n    "s24": "samsung_a35_flipkart_reviews_corrected.csv",\n    "iphone": "iphone_13_flipkart_reviews_corrected.csv"\n}\n\ndfs = {}\nfor k, fname in input_files.items():\n    try:\n        df = pd.read_csv(fname, encoding="utf-8")\n        print(f"Loaded {fname} -> {len(df)} rows")\n        dfs[k] = df\n    except FileNotFoundError:\n        print(f"File not found: {fname}. If your file has a different name, update input_files.")\n        dfs[k] = pd.DataFrame()\n\n\n# In[ ]:\n\n\n# 3) Helper cleaning functions\ndef clean_location(loc):\n    if pd.isna(loc): \n        return None\n    s = str(loc).strip()\n    # remove "Certified Buyer," or "Certified Buyer" prefix\n    s = re.sub(r\'(?i)certified buyer[,:\\s]*\', \'\', s).strip()\n    if s == "":\n        return None\n    return s\n\ndef parse_date_safe(x):\n    """Try to parse date; return pandas.Timestamp.date or NaT.\n       Discard dates outside allowed range (year < 2000 or > TODAY.year)."""\n    if pd.isna(x):\n        return pd.NaT\n    s = str(x).strip()\n    if s == "":\n        return pd.NaT\n    # remove common noise like \'Reviewed in India on \'\n    s = re.sub(r\'(?i)reviewed in [A-Za-z ,\\-()]+ on \', \'\', s).strip()\n    # try parse\n    try:\n        dt = dateparser.parse(s, fuzzy=True, dayfirst=False)\n        if not dt:\n            return pd.NaT\n        d = dt.date()\n        # sanity bounds\n        if d.year < EARLIEST_ALLOWED_YEAR or d > LATEST_ALLOWED_DATE:\n            return pd.NaT\n        return pd.Timestamp(d).date()\n    except Exception:\n        return pd.NaT\n\ndef normalize_verified(v):\n    # accepts booleans or strings like \'True\' or values from your scraper\n    if pd.isna(v):\n        return False\n    if isinstance(v, bool):\n        return v\n    s = str(v).strip().lower()\n    if s in ("true", "1", "yes", "certified buyer", "certified"):\n        return True\n    # check for \'certified buyer\' inside larger strings\n    if "certified buyer" in s:\n        return True\n    return False\n\ndef normalize_rating(x):\n    try:\n        if pd.isna(x):\n            return np.nan\n        # if float or int already\n        if isinstance(x, (int,float)):\n            return float(x)\n        s = str(x)\n        m = re.search(r\'(\\d+(\\.\\d+)?)\', s)\n        return float(m.group(1)) if m else np.nan\n    except:\n        return np.nan\n\n\n# In[ ]:\n\n\n# 4) Apply cleaning to each dataframe, produce cleaned CSV and debug artifacts\ncleaned = {}\nfor key, df in dfs.items():\n    if df is None or df.empty:\n        print(f"Skipping {key}: no data")\n        cleaned[key] = pd.DataFrame()\n        continue\n\n    df2 = df.copy()\n\n    # Standardize column names (lowercase)\n    df2.columns = [c.strip() for c in df2.columns]\n\n    # Ensure expected columns exist; if not, try common alternatives\n    colmap = {}\n    for c in df2.columns:\n        lc = c.lower()\n        if \'rating\' in lc and \'rating\' not in colmap.values():\n            colmap[c] = \'rating\'\n        elif \'title\' in lc and \'title\' not in colmap.values():\n            colmap[c] = \'title\'\n        elif \'review\' in lc and \'review_text\' not in colmap.values():\n            # prefer explicit \'review_text\', but match \'review\' or \'review_text\'\n            if \'review_text\' not in colmap.values():\n                colmap[c] = \'review_text\'\n        elif \'name\' in lc and \'reviewer_name\' not in colmap.values():\n            colmap[c] = \'reviewer_name\'\n        elif \'location\' in lc and \'reviewer_location\' not in colmap.values():\n            colmap[c] = \'reviewer_location\'\n        elif \'date\' in lc and \'review_date\' not in colmap.values():\n            colmap[c] = \'review_date\'\n        elif \'verified\' in lc or \'certified\' in lc:\n            colmap[c] = \'verified_purchase\'\n    df2 = df2.rename(columns=colmap)\n\n    # Ensure columns exist\n    for req in [\'rating\',\'title\',\'review_text\',\'reviewer_name\',\'reviewer_location\',\'review_date\',\'verified_purchase\']:\n        if req not in df2.columns:\n            df2[req] = np.nan\n\n    # Clean fields\n    df2[\'rating\'] = df2[\'rating\'].apply(normalize_rating)\n    df2[\'title\'] = df2[\'title\'].astype(str).replace(\'nan\',\'\').str.strip().replace(\'\', np.nan)\n    df2[\'review_text\'] = df2[\'review_text\'].astype(str).replace(\'nan\',\'\').str.strip().replace(\'\', np.nan)\n    df2[\'reviewer_name\'] = df2[\'reviewer_name\'].astype(str).replace(\'nan\',\'\').str.strip().replace(\'\', np.nan)\n    # location: remove \'Certified Buyer\' etc\n    df2[\'reviewer_location\'] = df2[\'reviewer_location\'].apply(clean_location)\n    # verified boolean\n    df2[\'verified_purchase\'] = df2[\'verified_purchase\'].apply(normalize_verified)\n    # parse dates safely\n    df2[\'parsed_review_date\'] = df2[\'review_date\'].apply(parse_date_safe)\n\n    # If parsed_review_date is NaT try extracting from review_text/title heuristically\n    mask_missing_date = df2[\'parsed_review_date\'].isna()\n    if mask_missing_date.any():\n        for idx in df2[mask_missing_date].index:\n            combined = " ".join(filter(None, [str(df2.at[idx,\'title\']), str(df2.at[idx,\'review_text\'])]))\n            guess = parse_date_safe(combined)\n            if not pd.isna(guess):\n                df2.at[idx,\'parsed_review_date\'] = guess\n\n    # Drop reviews with very short text (<10 chars) or no text\n    df2[\'text_len\'] = df2[\'review_text\'].fillna(\'\').str.len()\n    df2 = df2[df2[\'text_len\'] >= 10].copy()\n\n    # Deduplicate by review_text + rating\n    before = len(df2)\n    df2 = df2.drop_duplicates(subset=[\'review_text\',\'rating\']).reset_index(drop=True)\n    after = len(df2)\n\n    # Save cleaned CSV and a small debug for rows with missing/odd dates\n    cleaned_fname = f"{key}_cleaned.csv"\n    df2.to_csv(cleaned_fname, index=False, encoding="utf-8")\n    debug_dates = df2[df2[\'parsed_review_date\'].isna()].copy()\n    debug_fname = f"{key}_debug_missing_dates.csv"\n    debug_dates.to_csv(debug_fname, index=False, encoding="utf-8")\n\n    cleaned[key] = df2\n    print(f"{key}: before={len(dfs[key])}, after_filtering={before}, after_dedupe={after}. Clean saved -> {cleaned_fname}, debug missing dates -> {debug_fname}")\n\n\n# In[ ]:\n\n\n# 0. Install dependencies if needed (uncomment and run once)\nget_ipython().system(\'pip install nltk tqdm\')\n\n\n# In[ ]:\n\n\n# 1. Imports + config\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# Ensure required NLTK corpora are downloaded (runs silently if already present)\nnltk.download(\'punkt\', quiet=True)\nnltk.download(\'stopwords\', quiet=True)\nnltk.download(\'wordnet\', quiet=True)\nnltk.download(\'omw-1.4\', quiet=True)\n\nSTOPWORDS = set(stopwords.words(\'english\'))\nlemmatizer = WordNetLemmatizer()\n\n# Files saved by previous scraping step (edit if your filenames differ)\nINPUT_FILES = {\n    "samsung_a35": "samsung_a35_flipkart_reviews_corrected.csv",\n    "iphone_13":  "iphone_13_flipkart_reviews_corrected.csv"\n}\n\n# Output filenames\nOUTPUT_FILES = {k: f"{k}_cleaned_step1_2.csv" for k in INPUT_FILES.keys()}\n\n# Safety: small helper to show a snippet\ndef show_head(df, name, n=3):\n    print(f"\\n=== {name} head ({len(df)} rows) ===")\n    display(df.head(n))\n\n\n# In[ ]:\n\n\n# 2. Helper text-processing functions\nRE_PUNCT = re.compile(r"[{}]".format(re.escape(string.punctuation)))  # punctuation regex\n\ndef normalize_whitespace(text):\n    return re.sub(r"\\s+", " ", text).strip()\n\ndef preprocess_text_basic(text):\n    """Lowercase, remove extra whitespace, remove punctuation characters (not emojis), return text string."""\n    if pd.isna(text):\n        return ""\n    s = str(text)\n    s = s.lower()\n    # remove punctuation but preserve unicode letters/emojis: replace punctuation with space\n    s = RE_PUNCT.sub(" ", s)\n    s = normalize_whitespace(s)\n    return s\n\ndef tokenize(text):\n    # simple whitespace tokenizer after preprocess_text_basic\n    if not text:\n        return []\n    # nltk.word_tokenize could be used, but we use split for speed & clarity\n    toks = nltk.word_tokenize(text)\n    return toks\n\ndef remove_stopwords(tokens):\n    return [t for t in tokens if t not in STOPWORDS and len(t) > 1]\n\ndef lemmatize_tokens(tokens):\n    # use WordNetLemmatizer; do a simple pass\n    return [lemmatizer.lemmatize(t) for t in tokens]\n\ndef preprocess_pipeline(text):\n    """Full pipeline: lowercase -> punctuation removal -> tokenization -> stopword removal -> lemmatization"""\n    s = preprocess_text_basic(text)\n    toks = tokenize(s)\n    toks = [t for t in toks if re.search(r"[A-Za-z0-9]", t)]  # keep tokens that have alphanum\n    toks = remove_stopwords(toks)\n    toks = lemmatize_tokens(toks)\n    return toks\n\n# Quick test\nprint("STOPWORDS sample:", list(STOPWORDS)[:6])\nprint("Preprocess sample:", preprocess_pipeline("This is a sample review! Superb camera, battery okay."))\n\n\n# In[ ]:\n\n\n# 3. Cleaning & sentiment labeling function\ndef clean_and_label(df, text_col=\'review_text\', rating_col=\'rating\'):\n    """\n    - Drop rows with no review text\n    - Drop rows with missing ratings (we need the rating to label sentiment)\n    - Deduplicate by review_text + rating\n    - Create binary sentiment: rating >= 4 -> \'positive\'; rating <= 2 -> \'negative\'; rating == 3 -> \'neutral\' (we\'ll exclude later)\n    - Run text preprocessing pipeline and add columns:\n        \'text_clean\' = joined cleaned tokens (string)\n        \'tokens\' = list of tokens\n    """\n    df = df.copy()\n    # normalize column names\n    df.columns = [c.strip() for c in df.columns]\n    if text_col not in df.columns:\n        raise ValueError(f"Text column \'{text_col}\' not found in dataframe columns: {df.columns.tolist()}")\n    if rating_col not in df.columns:\n        raise ValueError(f"Rating column \'{rating_col}\' not found in dataframe columns: {df.columns.tolist()}")\n\n    # drop rows without text\n    df[text_col] = df[text_col].astype(str).replace(\'nan\', \'\')\n    df = df[df[text_col].str.strip().astype(bool)].copy()\n    # drop rows without rating\n    df = df[~df[rating_col].isna()].copy()\n\n    # normalize rating numeric (safe)\n    df[rating_col] = pd.to_numeric(df[rating_col], errors=\'coerce\')\n    df = df[~df[rating_col].isna()].copy()\n\n    # drop duplicates (exact text + rating)\n    before = len(df)\n    df = df.drop_duplicates(subset=[text_col, rating_col]).reset_index(drop=True)\n    after = len(df)\n    print(f"Dropped duplicates: {before-after} rows")\n\n    # create sentiment label\n    def rating_to_sentiment(r):\n        try:\n            r = float(r)\n        except:\n            return "neutral"\n        if r >= 4.0:\n            return "positive"\n        if r <= 2.0:\n            return "negative"\n        return "neutral"\n\n    df[\'sentiment_label_full\'] = df[rating_col].apply(rating_to_sentiment)\n\n    # Option: keep neutral rows if you want, but per instruction exclude neutral from binary dataset\n    # We\'ll create a filtered dataframe later.\n\n    # Preprocess text and produce tokens and cleaned text string\n    tokens_list = []\n    cleaned_texts = []\n    for txt in tqdm(df[text_col].astype(str).tolist(), desc="Preprocessing text"):\n        toks = preprocess_pipeline(txt)\n        tokens_list.append(toks)\n        cleaned_texts.append(" ".join(toks))\n\n    df[\'tokens\'] = tokens_list\n    df[\'text_clean\'] = cleaned_texts\n    df[\'text_len\'] = df[\'text_clean\'].apply(lambda x: len(str(x).split()))\n\n    # final: set verified_purchase to bool if exists\n    if \'verified_purchase\' in df.columns:\n        df[\'verified_purchase\'] = df[\'verified_purchase\'].apply(lambda v: bool(v) if not pd.isna(v) else False)\n\n    return df\n\n\n# In[ ]:\n\n\n# 4. Run cleaning pipeline on both files and save results\ncleaned_results = {}\nfor key, fname in INPUT_FILES.items():\n    try:\n        df_raw = pd.read_csv(fname, encoding=\'utf-8\')\n        print(f"\\nLoaded {fname}: {len(df_raw)} rows")\n    except FileNotFoundError:\n        print(f"File not found: {fname}. Skipping {key}.")\n        cleaned_results[key] = pd.DataFrame()\n        continue\n\n    # run cleaning and labeling\n    df_cleaned = clean_and_label(df_raw, text_col=\'review_text\', rating_col=\'rating\')\n\n    # EXCLUDE neutral (rating == 3) as per instruction (we still keep a copy with neutral if you want)\n    df_binary = df_cleaned[df_cleaned[\'sentiment_label_full\'] != \'neutral\'].copy()\n    # Map to final binary label column\n    df_binary[\'sentiment_binary\'] = df_binary[\'sentiment_label_full\'].map({\'positive\': 1, \'negative\': 0})\n\n    # Save two CSVs:\n    #  - full cleaned (includes neutrals)\n    #  - binary cleaned (only positive/negative)\n    out_full = OUTPUT_FILES[key]  # e.g., "samsung_a35_cleaned_step1_2.csv"\n    out_bin = out_full.replace(".csv", "_binary.csv")\n    df_cleaned.to_csv(out_full, index=False, encoding=\'utf-8\')\n    df_binary.to_csv(out_bin, index=False, encoding=\'utf-8\')\n\n    print(f"Saved cleaned full -> {out_full} ({len(df_cleaned)} rows)")\n    print(f"Saved cleaned binary -> {out_bin} ({len(df_binary)} rows)")\n\n    cleaned_results[key] = {\n        "full": df_cleaned,\n        "binary": df_binary\n    }\n\n    show_head(df_binary, f"{key} (binary) sample", n=4)\n\n\n# In[ ]:\n\n\n# 5. Quick checks and stats\nfor key, vals in cleaned_results.items():\n    if not vals:\n        continue\n    dfb = vals[\'binary\']\n    print(f"\\n--- Summary for {key} (binary dataset) ---")\n    print("Rows:", len(dfb))\n    print("Positive / Negative counts:")\n    print(dfb[\'sentiment_label_full\'].value_counts())\n    # top words overall (simple)\n    all_tokens = [t for toks in dfb[\'tokens\'].tolist() for t in toks]\n    top = Counter(all_tokens).most_common(20)\n    print("Top tokens:", top[:20])\n\n\n# In[ ]:\n\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style="whitegrid")\n\n# Load cleaned binary datasets\ndf_a35 = pd.read_csv("samsung_a35_cleaned_step1_2_binary.csv")\ndf_ip13 = pd.read_csv("iphone_13_cleaned_step1_2_binary.csv")\n\nprint("A35 Rows:", len(df_a35))\nprint("iPhone 13 Rows:", len(df_ip13))\n\ndf_a35.head()\n\n\n\n# In[ ]:\n\n\ndef dataset_summary(df, name):\n    print("\\n======================")\n    print(f"Summary for: {name}")\n    print("======================")\n    print(df.describe(include=\'all\'))\n    print("\\nSentiment distribution:")\n    print(df[\'sentiment_binary\'].value_counts())\n\ndataset_summary(df_a35, "Samsung A35")\ndataset_summary(df_ip13, "iPhone 13")\n\n\n# In[ ]:\n\n\ndef most_common_complaints(df, product_name, top_n=15):\n    neg_df = df[df[\'sentiment_binary\'] == 0]\n    all_tokens = [token for tokens in neg_df[\'tokens\'] for token in tokens]\n    top_words = Counter(all_tokens).most_common(top_n)\n\n    print(f"\\nðŸ”Ž Top {top_n} complaint keywords ({product_name}):")\n    for w, c in top_words:\n        print(f"{w}: {c}")\n\nmost_common_complaints(df_a35, "Samsung A35")\nmost_common_complaints(df_ip13, "iPhone 13")\n\n\n# In[ ]:\n\n\nissue_keywords = ["delivery", "late", "delay", "refund", "return", "replace", \n                  "battery", "heating", "quality", "broken", "damaged"]\n\ndef highlight_issue_percent(df, product_name):\n    text = df[\'text_clean\'].str.lower()\n    df[\'issue_flag\'] = text.apply(lambda x: any(k in x for k in issue_keywords))\n\n    issue_pct = df[\'issue_flag\'].mean() * 100\n    print(f"\\nðŸ“¦ Issue Mentions in {product_name}: {issue_pct:.2f}%")\n\nhighlight_issue_percent(df_a35, "Samsung A35")\nhighlight_issue_percent(df_ip13, "iPhone 13")\n\n\n# In[ ]:\n\n\ndef verified_vs_nonverified(df, product_name):\n    verified_stats = df.groupby("verified_purchase")[\'sentiment_binary\'].mean()\n\n    print(f"\\nðŸ›’ Sentiment (Verified vs Non-Verified) â€” {product_name}")\n    print(verified_stats)\n\n    sns.barplot(x=verified_stats.index.astype(str), y=verified_stats.values)\n    plt.title(f"{product_name}: Verified vs Non-Verified Buyers")\n    plt.ylabel("Positive Sentiment Rate")\n    plt.xlabel("Verified Purchase")\n    plt.show()\n\nverified_vs_nonverified(df_a35, "Samsung A35")\nverified_vs_nonverified(df_ip13, "iPhone 13")\n\n\n# In[ ]:\n\n\ndef review_length_insight(df, product_name):\n    plt.figure(figsize=(8,4))\n    sns.boxplot(x=df[\'sentiment_binary\'], y=df[\'text_len\'])\n    plt.xticks([0,1], ["Negative", "Positive"])\n    plt.ylabel("Review Word Count")\n    plt.title(f"Review Length vs Sentiment â€” {product_name}")\n    plt.show()\n\n    avg_pos = df[df[\'sentiment_binary\']==1][\'text_len\'].mean()\n    avg_neg = df[df[\'sentiment_binary\']==0][\'text_len\'].mean()\n\n    print(f"\\nðŸ“ Avg review length ({product_name}):")\n    print(f"Positive: {avg_pos:.2f} words")\n    print(f"Negative: {avg_neg:.2f} words")\n\nreview_length_insight(df_a35, "Samsung A35")\nreview_length_insight(df_ip13, "iPhone 13")\n\n\n# In[ ]:\n\n\n# Run only if libraries are missing\nget_ipython().system(\'pip install wordcloud matplotlib seaborn nltk\')\n\n\n# In[ ]:\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom collections import Counter\nimport nltk, re, os\nfrom PIL import Image\nfrom wordcloud import STOPWORDS\n\nsns.set(style="whitegrid")\nplt.rcParams.update({\'figure.max_open_warning\': 0})\n\n# path to the sample uploaded image (you provided earlier)\nsample_image_path = "/mnt/data/d97a1fcd-5fa1-4f0c-9140-4f1303b59e7a.png"\n\n\n# In[ ]:\n\n\n# Edit these names if your cleaned files have different names\nfile_a35 = "samsung_a35_cleaned_step1_2_binary.csv"\nfile_ip13 = "iphone_13_cleaned_step1_2_binary.csv"\n\n# load (fall back to other likely names if missing)\ndef try_load(names):\n    for n in names:\n        if os.path.exists(n):\n            return pd.read_csv(n)\n    return pd.DataFrame()\n\ndf_a35 = try_load([file_a35, "samsung_a35_cleaned_step1_2_binary.csv"])\ndf_ip13 = try_load([file_ip13, "iphone_13_cleaned_step1_2_binary.csv"])\n\nprint("Loaded rows:", "samsung_a35:", len(df_a35), "iphone_13:", len(df_ip13))\n# Choose one product to focus on or combine both\n# For dashboard we will show per-product charts; by default, use Samsung A35 if present.\ndf = df_a35 if not df_a35.empty else df_ip13\nproduct_label = "Samsung A35" if not df_a35.empty else "iPhone 13"\n\n\n# In[ ]:\n\n\n# ensure useful columns exist\nfor col in [\'sentiment_binary\',\'text_clean\',\'tokens\',\'review_date\',\'verified_purchase\',\'reviewer_location\',\'rating\',\'review_text\']:\n    if col not in df.columns:\n        df[col] = np.nan\n\n# quick conversion\ndf[\'sentiment_binary\'] = pd.to_numeric(df[\'sentiment_binary\'], errors=\'coerce\').fillna(0).astype(int)\ndf[\'review_date\'] = pd.to_datetime(df[\'review_date\'], errors=\'coerce\')\ndf[\'text_clean\'] = df[\'text_clean\'].fillna("")\n# tokens may be a string or list; normalize to list of tokens\ndef ensure_tokens(x):\n    if pd.isna(x): return []\n    if isinstance(x, list): return x\n    # if it\'s a string (space-separated)\n    if isinstance(x, str):\n        return [t for t in re.split(r"\\s+", x) if t]\n    return []\ndf[\'tokens\'] = df[\'tokens\'].apply(ensure_tokens)\n# text_len if missing\nif \'text_len\' not in df.columns:\n    df[\'text_len\'] = df[\'text_clean\'].str.split().apply(lambda x: len(x) if isinstance(x, list) or isinstance(x, str) else 0)\n\n\n# In[ ]:\n\n\nout_dir = "dashboard_outputs"\nos.makedirs(out_dir, exist_ok=True)\n\n# counts\ncounts = df[\'sentiment_binary\'].value_counts().sort_index()\nlabels = [\'Negative (0)\', \'Positive (1)\']\nvals = [counts.get(0,0), counts.get(1,0)]\n\n# bar histogram\nplt.figure(figsize=(6,4))\nsns.barplot(x=labels, y=vals)\nplt.title(f"Sentiment Distribution â€” {product_label}")\nplt.ylabel("Number of reviews")\nplt.xlabel("")\nplt.tight_layout()\nhist_path = os.path.join(out_dir, f"{product_label}_sentiment_hist.png")\nplt.savefig(hist_path, dpi=150)\nplt.show()\n\n# pie chart\nplt.figure(figsize=(5,5))\nplt.pie(vals, labels=labels, autopct=\'%1.1f%%\', startangle=140, explode=(0.05,0.05))\nplt.title(f"Sentiment Share â€” {product_label}")\npie_path = os.path.join(out_dir, f"{product_label}_sentiment_pie.png")\nplt.savefig(pie_path, dpi=150)\nplt.show()\nprint("Saved:", hist_path, pie_path)\n\n\n# In[ ]:\n\n\n# prepare monthly positive rate (proportion of positive reviews per month)\ndf_time = df.dropna(subset=[\'review_date\']).copy()\nif not df_time.empty:\n    df_time[\'month\'] = df_time[\'review_date\'].dt.to_period(\'M\').dt.to_timestamp()\n    monthly = df_time.groupby(\'month\')[\'sentiment_binary\'].agg([\'count\',\'mean\']).reset_index().sort_values(\'month\')\n    monthly[\'positive_rate_pct\'] = monthly[\'mean\'] * 100\n\n    plt.figure(figsize=(10,4))\n    sns.lineplot(data=monthly, x=\'month\', y=\'positive_rate_pct\', marker=\'o\')\n    plt.title(f"Monthly Positive Rate (%) â€” {product_label}")\n    plt.ylabel("Positive reviews (%)")\n    plt.xlabel("Month")\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    timeseries_path = os.path.join(out_dir, f"{product_label}_sentiment_trend.png")\n    plt.savefig(timeseries_path, dpi=150)\n    plt.show()\n    print("Saved:", timeseries_path)\nelse:\n    print("No valid review_date values available for time-series chart.")\n\n\n# In[ ]:\n\n\n# Prepare big text for positive and negative sets\npositive_text = " ".join(df[df[\'sentiment_binary\']==1][\'text_clean\'].fillna("").tolist())\nnegative_text = " ".join(df[df[\'sentiment_binary\']==0][\'text_clean\'].fillna("").tolist())\n\n# assemble stopwords\nwc_stop = set(STOPWORDS) | {"read","more","product","phone","buy","bought","flipkart","item","one","use","uses","using"}\n# create wordcloud function\ndef make_wordcloud(text, title, fname):\n    if not text.strip():\n        print("No text for", title)\n        return\n    wc = WordCloud(width=800, height=400, background_color="white", stopwords=wc_stop, collocations=False, max_words=150)\n    img = wc.generate(text)\n    plt.figure(figsize=(12,5))\n    plt.imshow(img, interpolation=\'bilinear\')\n    plt.axis(\'off\')\n    plt.title(title)\n    plt.tight_layout()\n    out = os.path.join(out_dir, fname)\n    plt.savefig(out, dpi=150)\n    plt.show()\n    print("Saved:", out)\n\nmake_wordcloud(positive_text, f"{product_label} â€” Positive Reviews Word Cloud", f"{product_label}_wordcloud_positive.png")\nmake_wordcloud(negative_text, f"{product_label} â€” Negative Reviews Word Cloud", f"{product_label}_wordcloud_negative.png")\n\n\n# In[ ]:\n\n\n# pain keywords to search for (expand as needed)\npain_keywords = ["battery","heat","heating","heating","charge","charging","delivery","refund","return","damage","broken",\n                 "screen","display","camera","sound","speaker","network","wifi","bluetooth","overheat","slow","lag",\n                 "price","value","durable","durability","update","bug"]\n\ndef top_pain_points(df, keywords, top_n=12):\n    text_series = df[\'text_clean\'].str.lower().fillna("")\n    counts = {}\n    for k in keywords:\n        # simple substring count across reviews (count reviews mentioning keyword)\n        counts[k] = text_series.apply(lambda s: 1 if k in s else 0).sum()\n    # sort\n    sorted_items = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n    top = sorted_items[:top_n]\n    keys = [k for k,v in top]\n    vals = [v for k,v in top]\n\n    plt.figure(figsize=(10,5))\n    sns.barplot(x=vals, y=keys)\n    plt.title(f"Top Pain Points Mentions â€” {product_label}")\n    plt.xlabel("Number of reviews mentioning")\n    plt.ylabel("")\n    plt.tight_layout()\n    out = os.path.join(out_dir, f"{product_label}_top_pain_points.png")\n    plt.savefig(out, dpi=150)\n    plt.show()\n    print("Saved:", out)\n    return dict(top)\n\npain_dict = top_pain_points(df, pain_keywords, top_n=12)\npain_dict\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n\n\n\n# In[ ]:\n\n\n\n\n'

def run_preprocessing(save_folder: str = "dashboard_outputs"):
    """Run the original preprocessing logic and save outputs to `save_folder`.
    Returns a list of generated file paths (CSV) inside save_folder.
    """
    save_folder = str(save_folder)
    os.makedirs(save_folder, exist_ok=True)
    # prepare a namespace where the original script will run
    ns = {}
    # provide commonly used variables
    ns['save_folder'] = save_folder
    # change cwd to script dir for relative paths if needed
    cwd = os.getcwd()
    try:
        script_dir = Path(__file__).parent
        os.chdir(script_dir)
        # execute the original script text in the namespace
        exec(_ORIG_SCRIPT, ns)
    finally:
        os.chdir(cwd)
    # detect CSVs created in save_folder
    created = sorted([str(p) for p in Path(save_folder).glob("*.csv")])
    return created

if __name__ == "__main__":
    print("Running preprocessing directly (this will execute original code). Outputs -> dashboard_outputs/")
    print(run_preprocessing(save_folder="dashboard_outputs"))
